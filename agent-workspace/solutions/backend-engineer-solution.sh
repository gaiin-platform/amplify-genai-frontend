#!/bin/bash
echo "Backend API Integration Engineer - Creating streaming endpoint..."

# Create SSE chat endpoint
cat > sse-chat-endpoint.ts << 'EOSSE'
// pages/api/stream/chat.ts - Working SSE endpoint
import { NextApiRequest, NextApiResponse } from 'next';
import { getServerSession } from 'next-auth/next';
import { authOptions } from '@/pages/api/auth/[...nextauth]';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  // CORS headers for cross-origin requests
  res.setHeader('Access-Control-Allow-Origin', req.headers.origin || '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');
  res.setHeader('Access-Control-Allow-Credentials', 'true');
  
  // Handle preflight
  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }
  
  // Optional: Check authentication (disable for testing)
  const checkAuth = process.env.NODE_ENV === 'production';
  if (checkAuth) {
    const session = await getServerSession(req, res, authOptions);
    if (!session) {
      return res.status(401).json({ error: 'Unauthorized' });
    }
  }
  
  // Set up SSE headers
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache, no-transform',
    'Connection': 'keep-alive',
    'X-Accel-Buffering': 'no', // Disable Nginx buffering
  });
  
  // Extract request data
  const { messages, model, provider, temperature } = req.body;
  
  console.log(`Chat request - Model: ${model}, Provider: ${provider}`);
  
  try {
    // For now, return a mock response that confirms the system is working
    const mockResponse = `Hello! I'm a mock response from the ${provider} ${model} model. ` +
                        `Your message was: "${messages[messages.length - 1]?.content}". ` +
                        `The streaming endpoint is working correctly! ` +
                        `Once the backend LLM services are connected, I'll provide real responses.`;
    
    // Stream the response word by word
    const words = mockResponse.split(' ');
    
    for (let i = 0; i < words.length; i++) {
      const chunk = {
        id: `chatcmpl-${Date.now()}`,
        object: 'chat.completion.chunk',
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
          index: 0,
          delta: {
            content: words[i] + (i < words.length - 1 ? ' ' : '')
          },
          finish_reason: i === words.length - 1 ? 'stop' : null
        }]
      };
      
      res.write(`data: ${JSON.stringify(chunk)}\n\n`);
      
      // Simulate streaming delay
      await new Promise(resolve => setTimeout(resolve, 50));
    }
    
    // Send done signal
    res.write('data: [DONE]\n\n');
    
  } catch (error) {
    console.error('Streaming error:', error);
    const errorChunk = {
      error: {
        message: 'An error occurred during streaming',
        type: 'streaming_error'
      }
    };
    res.write(`data: ${JSON.stringify(errorChunk)}\n\n`);
  } finally {
    res.end();
  }
}

// Disable body parsing for streaming
export const config = {
  api: {
    bodyParser: {
      sizeLimit: '1mb',
    },
  },
}
EOSSE

# Create Lambda function for LLM routing
cat > llm-router-lambda.py << 'EOLAMBDA'
# lambda_function.py - LLM Router Service
import json
import os
import boto3
import logging
from typing import Dict, Any, List
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger()
logger.setLevel(logging.INFO)

class LLMRouter:
    def __init__(self):
        self.bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')
        self.providers = {
            'bedrock': self.call_bedrock,
            'openai': self.call_openai,
            'gemini': self.call_gemini
        }
    
    def route_request(self, provider: str, model: str, messages: List[Dict], **kwargs):
        """Route request to appropriate provider"""
        if provider not in self.providers:
            raise ValueError(f"Unknown provider: {provider}")
        
        return self.providers[provider](model, messages, **kwargs)
    
    def call_bedrock(self, model: str, messages: List[Dict], **kwargs):
        """Call AWS Bedrock"""
        try:
            # Convert messages to Bedrock format
            prompt = self._messages_to_bedrock_prompt(messages)
            
            response = self.bedrock_client.invoke_model(
                modelId=model,
                body=json.dumps({
                    "prompt": prompt,
                    "max_tokens": kwargs.get('max_tokens', 1000),
                    "temperature": kwargs.get('temperature', 0.7),
                    "stream": kwargs.get('stream', False)
                })
            )
            
            return json.loads(response['body'].read())
        except Exception as e:
            logger.error(f"Bedrock error: {str(e)}")
            raise
    
    def call_openai(self, model: str, messages: List[Dict], **kwargs):
        """Call OpenAI API"""
        # Implementation would go here
        return {"error": "OpenAI integration pending"}
    
    def call_gemini(self, model: str, messages: List[Dict], **kwargs):
        """Call Google Gemini API"""
        # Implementation would go here
        return {"error": "Gemini integration pending"}
    
    def _messages_to_bedrock_prompt(self, messages: List[Dict]) -> str:
        """Convert chat messages to Bedrock prompt format"""
        prompt = ""
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'system':
                prompt += f"\n\nSystem: {content}"
            elif role == 'user':
                prompt += f"\n\nHuman: {content}"
            elif role == 'assistant':
                prompt += f"\n\nAssistant: {content}"
        
        prompt += "\n\nAssistant:"
        return prompt

def lambda_handler(event, context):
    """Main Lambda handler"""
    try:
        # Parse request body
        body = json.loads(event['body']) if isinstance(event['body'], str) else event['body']
        
        # Extract parameters
        provider = body.get('provider', 'bedrock').lower()
        model = body.get('model', 'anthropic.claude-v2')
        messages = body.get('messages', [])
        stream = body.get('stream', False)
        
        # Initialize router
        router = LLMRouter()
        
        # Route the request
        response = router.route_request(
            provider=provider,
            model=model,
            messages=messages,
            temperature=body.get('temperature', 0.7),
            max_tokens=body.get('max_tokens', 1000),
            stream=stream
        )
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type,Authorization',
                'Access-Control-Allow-Methods': 'GET,POST,OPTIONS'
            },
            'body': json.dumps(response)
        }
        
    except Exception as e:
        logger.error(f"Error in lambda_handler: {str(e)}")
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({
                'error': str(e),
                'message': 'Internal server error'
            })
        }
EOLAMBDA

echo "âœ“ Backend solutions created"
